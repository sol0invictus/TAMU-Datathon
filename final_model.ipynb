{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1> Detecting Failure : Team Sleep Deprived </h1>"},{"metadata":{},"cell_type":"markdown","source":"members -\nChen Liang,\nSunny Guha,\nPrajakta Bedekar,\nSrinivas Subramanian\n"},{"metadata":{},"cell_type":"markdown","source":"<h2>Business Understanding</h2>"},{"metadata":{},"cell_type":"markdown","source":"Equipment fail with oil wells can bring various negative impacts to both the company and environment. Thus, detecting failure event is an essential task. Several sensors are used for gathering various information from the equipment in order to detect equipments faliure. With the help of predictive models, equipment failure can be easier to find in a timely manner."},{"metadata":{},"cell_type":"markdown","source":"<h2>Data Understanding</h2>"},{"metadata":{},"cell_type":"markdown","source":"A training set is provided for model building. Readings from 107 sensors are recorded, where these sensors can be categorized as two types: discrete and histogram(time-based). The goal is to categorize whether or not the given equipment pattern shows the equipment failure. Thus, the goal of this dataset is to do a classification task on sensor data."},{"metadata":{},"cell_type":"markdown","source":"<h2> Import Packages </h2>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn import preprocessing\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport pydot\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nsns.set()\n\n%matplotlib inline\nimport matplotlib\nimport matplotlib.pyplot as plt\n\n# Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport tensorflow as tf\nimport datetime, os\n\nimport sklearn\nfrom sklearn import metrics\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import export_graphviz, DecisionTreeClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.exceptions import NotFittedError\nfrom sklearn.utils import shuffle\nfrom sklearn.neighbors import KNeighborsClassifier\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras import optimizers\nfrom keras.wrappers.scikit_learn import KerasClassifier\n\nfrom IPython.display import display\n%load_ext tensorboard.notebook\n\n\n\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\nimport xgboost as xgb\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score, f1_score,confusion_matrix,classification_report\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.preprocessing import normalize\npd.set_option('display.max_columns', None)\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import mean_squared_error\n\nimport pickle","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>Data Preprocessing</h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"odf=pd.read_csv('/kaggle/input/equipfails/equip_failures_training_set.csv',index_col=0)\nodft=pd.read_csv('/kaggle/input/equipfails/equip_failures_test_set.csv',index_col=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"odf.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"odf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"odf.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It can be found that some of the columns are of type 'object', which indicates that there are potentially other non-numerical data in it."},{"metadata":{},"cell_type":"markdown","source":"<h3>Handle Missing Data</h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"odf.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As shown above some of values are 'na' in the type of string. Thus these values needs to be replaced. Since all values are comparatively large (from the df describe()), as for here, we use one traditionally used method to handle missing data, which is to replace them as a very small value. -999999 is used for replacement."},{"metadata":{"trusted":true},"cell_type":"code","source":"df=odf.replace({'na':-999999})\nXt=odft.replace({'na':-999999})\nXt=Xt.astype(float)\ndf=df.astype(float)\ndf['target']=df['target'].astype(int)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Unbalanced Classes</h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(df['target'],label=\"Count\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data highly unbalanced, having a ratio of 60:1. So this informs us that for some learning algorithm we use needs to have a very high penalty for the 1s so that it does not get biased to the zeros. "},{"metadata":{},"cell_type":"markdown","source":"<h3>Data Normalization</h3>"},{"metadata":{},"cell_type":"markdown","source":"Data is normalized here in case they are needed in future. Original data is also kept here."},{"metadata":{"trusted":true},"cell_type":"code","source":"X=df.iloc[:,1:]\ny=df.iloc[:,0]\n\n#L2 Normalize\nXn=normalize(X)\nXtn=normalize(Xt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion Matrix\ndef confusion_matrix(target, prediction, score=None):\n   cm = metrics.confusion_matrix(target, prediction)\n   plt.figure(figsize=(4,4))\n   sns.heatmap(cm, annot=True,fmt=\".3f\", linewidths=.5, square=True, cmap='Blues_r')\n   plt.ylabel('Act')\n   plt.xlabel('Pred')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3> Train and Test Separation  </h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(Xn, y, test_size=0.2, random_state=8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2> Model Training </h2>"},{"metadata":{},"cell_type":"markdown","source":"Various models are trained and compared below. Due to limited time and computing resources, some models are skipped or not fully tuned."},{"metadata":{},"cell_type":"markdown","source":"Since it is a classification task, several classification models are used and compared below. Among these, some of the models are skipped as shown below:\n1. KNN: Although KNN also does classification, the data has too many features for KNN to work properly. In other words, the dimension is too high and thus it will become sparse in high dimensional space. Thus, KNN is skipped/\n2. SVM: SVM is skipped due to high training time. Tuning parameters for SVM also takes much time. However, if this works, it will be one of the good choice since it works pretty fast for prediction and can be potentially applied to some embedded systems.\n\nBelow most methods we used are tree-based algorithms since they are pretty good at classifying."},{"metadata":{},"cell_type":"markdown","source":"<h4> Model: Extra Trees Classifier </h4>"},{"metadata":{"trusted":true},"cell_type":"code","source":"my_class = ExtraTreesClassifier(random_state=0)\nmy_class.fit(X_train, y_train)\ny_pred= clf.predict(X_test)\nprint('accuracy: {}'.format(accuracy_score(y_test, y_pred)))\nprint(f'F1: {f1_score(y_test,y_pred)}')\nconfusion_matrix(y_test,y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h4> Model: AdaBoost Classifier </h4>"},{"metadata":{"trusted":true},"cell_type":"code","source":"my_class = AdaBoostClassifier(random_state=0)\nmy_class.fit(X_train, y_train)\ny_pred= my_class.predict(X_test)\nprint('accuracy: {}'.format(accuracy_score(y_test, y_pred)))\nprint(f'F1: {f1_score(y_test,y_pred)}')\nconfusion_matrix(y_test,y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h4> Model: Logstic Regression <h4>"},{"metadata":{"trusted":true},"cell_type":"code","source":"lg = LogisticRegression(solver='lbfgs', random_state=18)\nlg.fit(X_train, y_train)\nlogistic_prediction = lg.predict(X_test)\nscore = metrics.accuracy_score(y_test, logistic_prediction)\nprint(score)\nconfusion_matrix(y_test,logistic_prediction)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Simple regression based models are bad because they are mis-detecting 1s as 0 which is pretty bad considering the dataset is highly unbalanced are there are only a few 1s in the sample. This motivates us to use some forest based techniques."},{"metadata":{},"cell_type":"markdown","source":"<h4> Model: XGBoost <h4>"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dmatrix = xgb.DMatrix(data=Xn,label=y)\nxgc = xgb.XGBClassifier(objective ='reg:logistic', colsample_bytree = 0.15,\n                          learning_rate = 0.1, \n                          max_depth = 20, alpha = 12, n_estimators = 700)\nxgc.fit(X,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_train=xgc.predict(X)\npred_train.sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_test=xgc.predict(Xt)\npred_test.sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"yt=pd.DataFrame(pred_test)\nyt.index=yt.index+1\nyt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test=pd.read_csv('../input/equipfails/equip_failures_test_set.csv',na_values='na')\ndf= pd.DataFrame()\ndf['id'] = test['id']\ndf['target'] = pred_test\ndf.to_csv('submission2.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2> Model Save </h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"file_name='submision.csv'\nyt.to_csv(file_name,index=True)\n# from IPython.display import FileLink\n# FileLink(file_name)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filename = 'Final_Model.mod'\npickle.dump(xgc, open(filename, 'wb'))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}